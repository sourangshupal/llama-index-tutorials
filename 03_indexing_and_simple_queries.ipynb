{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Indexing & Simple Queries\n",
    "\n",
    "**Difficulty:** Intermediate | **Estimated Time:** 90-120 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. ‚úÖ Integrate external vector stores (Qdrant, Chroma)\n",
    "2. ‚úÖ Compare embedding models (OpenAI vs HuggingFace)\n",
    "3. ‚úÖ Persist and load indexes from storage\n",
    "4. ‚úÖ Configure query engines with different modes\n",
    "5. ‚úÖ Implement VectorIndexRetriever and VectorIndexAutoRetriever\n",
    "6. ‚úÖ Understand response synthesis modes\n",
    "7. ‚úÖ Implement streaming responses\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Notebooks 1 & 2\n",
    "- Understanding of vector similarity and embeddings\n",
    "- Qdrant or Chroma installed (optional - in-memory works too)\n",
    "\n",
    "## Curriculum Coverage\n",
    "\n",
    "- **Section 3.1:** Vector Store Integration\n",
    "- **Section 3.2:** Creating VectorStoreIndex\n",
    "- **Section 3.3:** Embedding Models\n",
    "- **Section 3.4:** Query Engines\n",
    "- **Section 3.5.1-3.5.2:** VectorIndexRetriever, VectorIndexAutoRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Core LlamaIndex\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    Settings,\n",
    "    Document,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.core.vector_stores import VectorStoreInfo, MetadataInfo\n",
    "from llama_index.core.retrievers import VectorIndexRetriever, VectorIndexAutoRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# Vector Stores\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "try:\n",
    "    from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "    QDRANT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    QDRANT_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  Qdrant not installed. Install with: pip install llama-index-vector-stores-qdrant qdrant-client\")\n",
    "\n",
    "# Embeddings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# LLM\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# External libraries\n",
    "import chromadb\n",
    "if QDRANT_AVAILABLE:\n",
    "    from qdrant_client import QdrantClient\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Settings configured\n"
     ]
    }
   ],
   "source": [
    "# Load environment and configure Settings\n",
    "load_dotenv()\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "Settings.embed_model = OpenAIEmbedding(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    dimensions=1536\n",
    ")\n",
    "Settings.chunk_size = 1024\n",
    "Settings.chunk_overlap = 200\n",
    "\n",
    "print(\"‚úÖ Settings configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Prepare Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created 5 sample documents\n",
      "   Topics: chroma, vector_databases, qdrant, algorithms, embeddings\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive sample documents\n",
    "documents = [\n",
    "    Document(\n",
    "        text=\"\"\"\n",
    "        Vector databases are specialized databases designed to store and query high-dimensional vectors.\n",
    "        These vectors typically represent embeddings of text, images, or other data. Vector databases\n",
    "        enable efficient similarity search using algorithms like HNSW (Hierarchical Navigable Small World)\n",
    "        or IVF (Inverted File Index). Popular vector databases include Qdrant, Pinecone, Weaviate, and Milvus.\n",
    "        \"\"\",\n",
    "        metadata={\"topic\": \"vector_databases\", \"difficulty\": \"intermediate\", \"year\": 2023}\n",
    "    ),\n",
    "    Document(\n",
    "        text=\"\"\"\n",
    "        HNSW (Hierarchical Navigable Small World) is a graph-based algorithm for approximate nearest neighbor\n",
    "        search. It builds a multi-layer graph where each layer is a subset of the previous one. The algorithm\n",
    "        achieves excellent query performance (sub-millisecond) with high recall. HNSW parameters include\n",
    "        M (number of connections per node) and ef_construction (search width during construction).\n",
    "        \"\"\",\n",
    "        metadata={\"topic\": \"algorithms\", \"difficulty\": \"advanced\", \"year\": 2023}\n",
    "    ),\n",
    "    Document(\n",
    "        text=\"\"\"\n",
    "        Embedding models convert text into dense vector representations that capture semantic meaning.\n",
    "        OpenAI's text-embedding-3-small produces 1536-dimensional vectors and is optimized for retrieval tasks.\n",
    "        Open-source alternatives include sentence-transformers models like all-MiniLM-L6-v2 (384 dimensions)\n",
    "        and all-mpnet-base-v2 (768 dimensions). The choice of embedding model affects retrieval quality and cost.\n",
    "        \"\"\",\n",
    "        metadata={\"topic\": \"embeddings\", \"difficulty\": \"beginner\", \"year\": 2024}\n",
    "    ),\n",
    "    Document(\n",
    "        text=\"\"\"\n",
    "        Qdrant is an open-source vector database written in Rust. It supports HNSW indexing, filtering,\n",
    "        and hybrid search. Qdrant can run locally (Docker) or in the cloud. Key features include payload\n",
    "        filtering, quantization for memory reduction, and distributed deployments. Qdrant is particularly\n",
    "        well-suited for production RAG applications.\n",
    "        \"\"\",\n",
    "        metadata={\"topic\": \"qdrant\", \"difficulty\": \"intermediate\", \"year\": 2024}\n",
    "    ),\n",
    "    Document(\n",
    "        text=\"\"\"\n",
    "        Chroma is a lightweight, embedded vector database designed for AI applications. It runs in-memory\n",
    "        or can persist to disk. Chroma is easy to set up and integrates seamlessly with LangChain and LlamaIndex.\n",
    "        It's ideal for prototyping and small-to-medium scale applications. Chroma supports metadata filtering\n",
    "        and multiple distance metrics (cosine, euclidean, dot product).\n",
    "        \"\"\",\n",
    "        metadata={\"topic\": \"chroma\", \"difficulty\": \"beginner\", \"year\": 2024}\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Created {len(documents)} sample documents\")\n",
    "print(f\"   Topics: {', '.join(set(d.metadata['topic'] for d in documents))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. In-Memory Vector Store (Default)\n",
    "\n",
    "### SimpleVectorStore: LlamaIndex's Built-in Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating VectorStoreIndex (in-memory)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8d80bf9bd044a480e9a3fa6097b133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dff7a42d9b6452ea78943fa3d9a5347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Index created in 1.76 seconds\n",
      "   Vector store type: SimpleVectorStore (in-memory)\n"
     ]
    }
   ],
   "source": [
    "# Create index with default in-memory vector store\n",
    "print(\"Creating VectorStoreIndex (in-memory)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "simple_index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n‚úÖ Index created in {elapsed:.2f} seconds\")\n",
    "print(f\"   Vector store type: SimpleVectorStore (in-memory)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use In-Memory Vector Store\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ No external dependencies\n",
    "- ‚úÖ Fast setup\n",
    "- ‚úÖ Good for prototyping\n",
    "- ‚úÖ Can persist to disk\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå Not optimized for large-scale (>100k vectors)\n",
    "- ‚ùå Limited filtering capabilities\n",
    "- ‚ùå No distributed support\n",
    "- ‚ùå Slower than specialized vector DBs\n",
    "\n",
    "**Use for**: Demos, small datasets, development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Chroma Vector Store Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating VectorStoreIndex with Chroma...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540ad63e51d2458cb7afcac3c9afed74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad128e96e16248b9aae16385cd847661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Chroma index created in 1.77 seconds\n",
      "   Collection: llama_index_docs\n",
      "   Documents indexed: 5\n"
     ]
    }
   ],
   "source": [
    "# Initialize Chroma client (in-memory)\n",
    "chroma_client = chromadb.EphemeralClient()  # In-memory\n",
    "# For persistence: chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# Create collection\n",
    "collection_name = \"llama_index_docs\"\n",
    "chroma_collection = chroma_client.create_collection(collection_name)\n",
    "\n",
    "# Create Chroma vector store\n",
    "chroma_vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# Create storage context\n",
    "storage_context = StorageContext.from_defaults(vector_store=chroma_vector_store)\n",
    "\n",
    "print(\"Creating VectorStoreIndex with Chroma...\")\n",
    "start_time = time.time()\n",
    "\n",
    "chroma_index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n‚úÖ Chroma index created in {elapsed:.2f} seconds\")\n",
    "print(f\"   Collection: {collection_name}\")\n",
    "print(f\"   Documents indexed: {chroma_collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chroma Features\n",
    "\n",
    "- **Ease of use**: Zero configuration for local dev\n",
    "- **Metadata filtering**: WHERE clause support\n",
    "- **Distance metrics**: Cosine (default), L2, IP\n",
    "- **Persistence**: Optional disk storage\n",
    "- **Scales to**: ~1M vectors comfortably"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Qdrant Vector Store Integration (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating VectorStoreIndex with Qdrant...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75fc0457fc3548bda7831a2280bc7707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "745802426899485b888998a7beec949d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Qdrant index created in 1.95 seconds\n",
      "   Collection: llama_index_qdrant\n"
     ]
    }
   ],
   "source": [
    "if QDRANT_AVAILABLE:\n",
    "    # Initialize Qdrant client (in-memory)\n",
    "    qdrant_client = QdrantClient(location=\":memory:\")\n",
    "    # For persistence: QdrantClient(path=\"./qdrant_db\")\n",
    "    # For cloud: QdrantClient(url=os.getenv(\"QDRANT_URL\"), api_key=os.getenv(\"QDRANT_API_KEY\"))\n",
    "    \n",
    "    # Create Qdrant vector store\n",
    "    qdrant_vector_store = QdrantVectorStore(\n",
    "        client=qdrant_client,\n",
    "        collection_name=\"llama_index_qdrant\",\n",
    "    )\n",
    "    \n",
    "    # Create storage context\n",
    "    qdrant_storage_context = StorageContext.from_defaults(vector_store=qdrant_vector_store)\n",
    "    \n",
    "    print(\"Creating VectorStoreIndex with Qdrant...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    qdrant_index = VectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        storage_context=qdrant_storage_context,\n",
    "        show_progress=True,\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n‚úÖ Qdrant index created in {elapsed:.2f} seconds\")\n",
    "    print(f\"   Collection: llama_index_qdrant\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping Qdrant example (not installed)\")\n",
    "    qdrant_index = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ ML Engineering Note: Vector Database Comparison\n",
    "\n",
    "| Feature | SimpleVectorStore | Chroma | Qdrant |\n",
    "|---------|------------------|--------|--------|\n",
    "| **Setup** | Built-in | Easy | Moderate |\n",
    "| **Scale** | <10k vectors | ~1M vectors | 10M+ vectors |\n",
    "| **Speed** | Moderate | Fast | Very Fast |\n",
    "| **Filtering** | Basic | Good | Excellent |\n",
    "| **Hybrid Search** | No | No | Yes |\n",
    "| **Cloud Option** | No | Planned | Yes |\n",
    "| **Best For** | Prototyping | Small-medium apps | Production |\n",
    "\n",
    "**Recommendation**: \n",
    "- Prototyping: SimpleVectorStore or Chroma\n",
    "- Production (<1M docs): Chroma or Qdrant\n",
    "- Production (>1M docs): Qdrant, Pinecone, or Weaviate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Embedding Model Comparison\n",
    "\n",
    "### 6.1 OpenAI Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Embedding (text-embedding-3-small):\n",
      "  Dimensions: 1536\n",
      "  Time: 501.56ms\n",
      "  First 5 values: [-0.02636837400496006, 0.040111102163791656, 0.008521012030541897, -0.012482762336730957, -0.007683198899030685]\n"
     ]
    }
   ],
   "source": [
    "# Test OpenAI embedding\n",
    "openai_embed = OpenAIEmbedding(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    dimensions=1536,\n",
    ")\n",
    "\n",
    "test_text = \"Vector databases enable semantic search\"\n",
    "start_time = time.time()\n",
    "openai_vector = openai_embed.get_text_embedding(test_text)\n",
    "openai_time = time.time() - start_time\n",
    "\n",
    "print(f\"OpenAI Embedding (text-embedding-3-small):\")\n",
    "print(f\"  Dimensions: {len(openai_vector)}\")\n",
    "print(f\"  Time: {openai_time*1000:.2f}ms\")\n",
    "print(f\"  First 5 values: {openai_vector[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 HuggingFace Embeddings (Local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HuggingFace model (this may take a moment on first run)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf8fc509d624172bad90969914cba26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945870613f1849fd803dcaab7ac9f580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cbbd6addd924fa38cbaf12df2ceb836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0bac7c149234ec48b9ab44e04c402e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc897b4e1d3249bda95ec3a70895ed5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247b48e30dd14d64b14c2e664802e485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9088f842668d4cf9932a073fd9ffea34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20315f0e12eb405c802da1b638176a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d679a4fa60547fc88a4281b2bb7bacb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64844e4378434624bd19c7b8611890ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d15b1a00a14c30b603a4147982a929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HuggingFace Embedding (all-MiniLM-L6-v2):\n",
      "  Dimensions: 384\n",
      "  Time: 2593.42ms\n",
      "  First 5 values: [0.04381900280714035, -0.009547049179673195, -0.020091239362955093, 0.01502758264541626, 0.01334494911134243]\n"
     ]
    }
   ],
   "source": [
    "# Test HuggingFace embedding\n",
    "print(\"Loading HuggingFace model (this may take a moment on first run)...\")\n",
    "hf_embed = HuggingFaceEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",  # 384 dimensions\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "hf_vector = hf_embed.get_text_embedding(test_text)\n",
    "hf_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nHuggingFace Embedding (all-MiniLM-L6-v2):\")\n",
    "print(f\"  Dimensions: {len(hf_vector)}\")\n",
    "print(f\"  Time: {hf_time*1000:.2f}ms\")\n",
    "print(f\"  First 5 values: {hf_vector[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding Model Comparison:\n",
      "                        Model  Dimensions Time (ms)            Cost   Quality Hosting\n",
      "OpenAI text-embedding-3-small        1536    501.56 $0.02/1M tokens Excellent     API\n",
      "             all-MiniLM-L6-v2         384   2593.42            Free      Good   Local\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comparison = pd.DataFrame([\n",
    "    {\n",
    "        \"Model\": \"OpenAI text-embedding-3-small\",\n",
    "        \"Dimensions\": len(openai_vector),\n",
    "        \"Time (ms)\": f\"{openai_time*1000:.2f}\",\n",
    "        \"Cost\": \"$0.02/1M tokens\",\n",
    "        \"Quality\": \"Excellent\",\n",
    "        \"Hosting\": \"API\",\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"all-MiniLM-L6-v2\",\n",
    "        \"Dimensions\": len(hf_vector),\n",
    "        \"Time (ms)\": f\"{hf_time*1000:.2f}\",\n",
    "        \"Cost\": \"Free\",\n",
    "        \"Quality\": \"Good\",\n",
    "        \"Hosting\": \"Local\",\n",
    "    },\n",
    "])\n",
    "\n",
    "print(\"\\nEmbedding Model Comparison:\")\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ ML Engineering Note: Embedding Selection\n",
    "\n",
    "**OpenAI (text-embedding-3-small):**\n",
    "- ‚úÖ State-of-the-art quality\n",
    "- ‚úÖ Variable dimensions (256-1536)\n",
    "- ‚úÖ No model hosting needed\n",
    "- ‚ùå Costs per API call\n",
    "- ‚ùå Data leaves your infrastructure\n",
    "\n",
    "**HuggingFace (all-MiniLM-L6-v2):**\n",
    "- ‚úÖ Free and open-source\n",
    "- ‚úÖ Runs locally (data privacy)\n",
    "- ‚úÖ Fast inference (especially with GPU)\n",
    "- ‚ùå Lower quality than OpenAI\n",
    "- ‚ùå Fixed dimensions (384)\n",
    "- ‚ùå Requires model hosting\n",
    "\n",
    "**Recommendation**: \n",
    "- Development: OpenAI (fast iteration)\n",
    "- Production (high quality needed): OpenAI\n",
    "- Production (cost-sensitive, privacy): HuggingFace + GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Index Persistence\n",
    "\n",
    "### Saving Index to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persisting index to ./storage...\n",
      "\n",
      "‚úÖ Index persisted successfully!\n",
      "   Location: ./storage\n",
      "   Files created: 5\n",
      "     - image__vector_store.json\n",
      "     - graph_store.json\n",
      "     - index_store.json\n",
      "     - docstore.json\n",
      "     - default__vector_store.json\n"
     ]
    }
   ],
   "source": [
    "# Save index to disk\n",
    "persist_dir = \"./storage\"\n",
    "\n",
    "print(f\"Persisting index to {persist_dir}...\")\n",
    "simple_index.storage_context.persist(persist_dir=persist_dir)\n",
    "\n",
    "print(\"\\n‚úÖ Index persisted successfully!\")\n",
    "print(f\"   Location: {persist_dir}\")\n",
    "\n",
    "# Check what was saved\n",
    "storage_path = Path(persist_dir)\n",
    "if storage_path.exists():\n",
    "    files = list(storage_path.glob(\"*\"))\n",
    "    print(f\"   Files created: {len(files)}\")\n",
    "    for f in files:\n",
    "        print(f\"     - {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Index from Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading index from ./storage...\n",
      "‚úÖ Index loaded successfully!\n",
      "\n",
      "Test query on loaded index:\n",
      "  Query: What is Qdrant?\n",
      "  Response: Qdrant is an open-source vector database that is implemented in Rust. It features HNSW indexing, filtering capabilities, and supports hybrid search. Qdrant can be deployed locally using Docker or in cloud environments. Its key functionalities include payload filtering, quantization for memory efficiency, and support for distributed deployments, making it particularly suitable for production applications involving retrieval-augmented generation (RAG).\n"
     ]
    }
   ],
   "source": [
    "# Load index from disk\n",
    "print(f\"Loading index from {persist_dir}...\")\n",
    "\n",
    "storage_context_loaded = StorageContext.from_defaults(persist_dir=persist_dir)\n",
    "loaded_index = load_index_from_storage(storage_context_loaded)\n",
    "\n",
    "print(\"‚úÖ Index loaded successfully!\")\n",
    "\n",
    "# Test the loaded index\n",
    "test_query_engine = loaded_index.as_query_engine(similarity_top_k=2)\n",
    "test_response = test_query_engine.query(\"What is Qdrant?\")\n",
    "\n",
    "print(f\"\\nTest query on loaded index:\")\n",
    "print(f\"  Query: What is Qdrant?\")\n",
    "print(f\"  Response: {test_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persistence Best Practices\n",
    "\n",
    "1. **Version your indexes**: Include timestamp or version in directory name\n",
    "2. **Backup regularly**: Especially before re-indexing\n",
    "3. **Separate storage by environment**: dev/staging/prod\n",
    "4. **Monitor disk usage**: Indexes can grow large\n",
    "5. **Use external vector DBs for production**: Better than file-based persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Query Engine Configuration\n",
    "\n",
    "### 8.1 Basic Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the main vector databases mentioned?\n",
      "\n",
      "Response:\n",
      "The main vector databases mentioned are Qdrant, Pinecone, Weaviate, and Milvus.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Sources used: 3\n"
     ]
    }
   ],
   "source": [
    "# Create query engine with configuration\n",
    "query_engine = chroma_index.as_query_engine(\n",
    "    similarity_top_k=3,\n",
    "    response_mode=\"compact\",\n",
    ")\n",
    "\n",
    "query = \"What are the main vector databases mentioned?\"\n",
    "response = query_engine.query(query)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(f\"Response:\\n{response}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"\\nSources used: {len(response.source_nodes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Response Synthesis Modes Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing response modes with query: 'Explain HNSW algorithm'\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Mode: compact\n",
      "  Time: 4.39s\n",
      "  Response length: 610 chars\n",
      "  Response: HNSW, or Hierarchical Navigable Small World, is a graph-based algorithm designed for approximate nearest neighbor search. It constructs a multi-layer graph where each layer is a subset of the previous...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Mode: tree_summarize\n",
      "  Time: 3.96s\n",
      "  Response length: 618 chars\n",
      "  Response: HNSW, or Hierarchical Navigable Small World, is a graph-based algorithm designed for approximate nearest neighbor search. It constructs a multi-layer graph where each layer represents a subset of the ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Mode: simple_summarize\n",
      "  Time: 3.51s\n",
      "  Response length: 588 chars\n",
      "  Response: HNSW (Hierarchical Navigable Small World) is a graph-based algorithm designed for approximate nearest neighbor search. It constructs a multi-layer graph, where each layer is a subset of the previous o...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Mode: refine\n",
      "  Time: 7.47s\n",
      "  Response length: 748 chars\n",
      "  Response: HNSW, or Hierarchical Navigable Small World, is a graph-based algorithm specifically designed for approximate nearest neighbor search. It builds a multi-layer graph, with each layer representing a sub...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test all response modes\n",
    "modes = [\"compact\", \"tree_summarize\", \"simple_summarize\", \"refine\"]\n",
    "test_query = \"Explain HNSW algorithm\"\n",
    "\n",
    "print(f\"Testing response modes with query: '{test_query}'\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for mode in modes:\n",
    "    engine = chroma_index.as_query_engine(\n",
    "        similarity_top_k=2,\n",
    "        response_mode=mode\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    resp = engine.query(test_query)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"\\nMode: {mode}\")\n",
    "    print(f\"  Time: {elapsed:.2f}s\")\n",
    "    print(f\"  Response length: {len(str(resp))} chars\")\n",
    "    print(f\"  Response: {str(resp)[:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Mode Characteristics\n",
    "\n",
    "| Mode | Speed | Quality | Best For |\n",
    "|------|-------|---------|----------|\n",
    "| **compact** | Fast | Good | General use |\n",
    "| **tree_summarize** | Moderate | Excellent | Long contexts |\n",
    "| **simple_summarize** | Very Fast | Basic | Simple queries |\n",
    "| **refine** | Slow | Excellent | High quality needed |\n",
    "| **accumulate** | Slow | Varied | Multiple perspectives |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Streaming Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the difference between Qdrant and Chroma?\n",
      "\n",
      "Streaming response:\n",
      "--------------------------------------------------------------------------------\n",
      "Qdrant and Chroma are both vector databases, but they cater to different needs and use cases. Chroma is lightweight and designed for easy setup, making it suitable for prototyping and small-to-medium scale applications. It runs in-memory or can persist to disk and integrates well with tools like LangChain and LlamaIndex. Chroma supports metadata filtering and various distance metrics.\n",
      "\n",
      "On the other hand, Qdrant is an open-source database written in Rust, offering more advanced features such as HNSW indexing, filtering, and hybrid search. It can be deployed locally using Docker or in the cloud and is optimized for production applications, particularly in retrieval-augmented generation (RAG) scenarios. Qdrant also includes features like payload filtering, quantization for memory efficiency, and support for distributed deployments.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create streaming query engine\n",
    "streaming_engine = chroma_index.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "query = \"What is the difference between Qdrant and Chroma?\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Streaming response:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "response = streaming_engine.query(query)\n",
    "\n",
    "# Stream tokens\n",
    "for text in response.response_gen:\n",
    "    print(text, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Streaming?\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ **Better UX**: Users see progress immediately\n",
    "- ‚úÖ **Lower perceived latency**: First token arrives faster\n",
    "- ‚úÖ **Interruptible**: Can stop generation early\n",
    "\n",
    "**Use cases:**\n",
    "- Chatbots and conversational interfaces\n",
    "- Long-form content generation\n",
    "- User-facing applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. VectorIndexRetriever\n",
    "\n",
    "### Manual Retrieval Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: vector database algorithms\n",
      "\n",
      "Retrieved 3 nodes:\n",
      "\n",
      "Node 1:\n",
      "  Score: 0.4810\n",
      "  Topic: vector_databases\n",
      "  Difficulty: intermediate\n",
      "  Text (first 150 chars): Vector databases are specialized databases designed to store and query high-dimensional vectors.\n",
      "        These vectors typically represent embeddings ...\n",
      "\n",
      "Node 2:\n",
      "  Score: 0.2966\n",
      "  Topic: algorithms\n",
      "  Difficulty: advanced\n",
      "  Text (first 150 chars): HNSW (Hierarchical Navigable Small World) is a graph-based algorithm for approximate nearest neighbor\n",
      "        search. It builds a multi-layer graph wh...\n",
      "\n",
      "Node 3:\n",
      "  Score: 0.2869\n",
      "  Topic: qdrant\n",
      "  Difficulty: intermediate\n",
      "  Text (first 150 chars): Qdrant is an open-source vector database written in Rust. It supports HNSW indexing, filtering,\n",
      "        and hybrid search. Qdrant can run locally (Doc...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create custom retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=chroma_index,\n",
    "    similarity_top_k=3,\n",
    ")\n",
    "\n",
    "# Retrieve nodes directly (no LLM synthesis)\n",
    "query_str = \"vector database algorithms\"\n",
    "retrieved_nodes = retriever.retrieve(query_str)\n",
    "\n",
    "print(f\"Query: {query_str}\\n\")\n",
    "print(f\"Retrieved {len(retrieved_nodes)} nodes:\\n\")\n",
    "\n",
    "for i, node in enumerate(retrieved_nodes, 1):\n",
    "    print(f\"Node {i}:\")\n",
    "    print(f\"  Score: {node.score:.4f}\")\n",
    "    print(f\"  Topic: {node.metadata.get('topic')}\")\n",
    "    print(f\"  Difficulty: {node.metadata.get('difficulty')}\")\n",
    "    print(f\"  Text (first 150 chars): {node.text[:150]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Query Engine from Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from custom retriever:\n",
      "HNSW, or Hierarchical Navigable Small World, is a graph-based algorithm designed for approximate nearest neighbor search. It constructs a multi-layer graph where each layer is a subset of the previous one, allowing for efficient navigation through the data. The algorithm is known for its excellent query performance, achieving response times in the sub-millisecond range while maintaining high recall rates. Key parameters of HNSW include M, which determines the number of connections per node, and ef_construction, which defines the search width during the construction phase of the graph. This structure enables effective and rapid similarity searches in high-dimensional spaces.\n"
     ]
    }
   ],
   "source": [
    "# Build query engine from custom retriever\n",
    "custom_query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=retriever,\n",
    "    response_mode=\"compact\",\n",
    ")\n",
    "\n",
    "response = custom_query_engine.query(\"Explain the HNSW algorithm\")\n",
    "print(f\"Response from custom retriever:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. VectorIndexAutoRetriever\n",
    "\n",
    "### Natural Language Metadata Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ VectorIndexAutoRetriever configured\n"
     ]
    }
   ],
   "source": [
    "# Define metadata schema for auto-retriever\n",
    "vector_store_info = VectorStoreInfo(\n",
    "    content_info=\"Technical documentation about vector databases and embeddings\",\n",
    "    metadata_info=[\n",
    "        MetadataInfo(\n",
    "            name=\"topic\",\n",
    "            type=\"str\",\n",
    "            description=\"The main topic of the document (e.g., 'qdrant', 'chroma', 'embeddings')\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"difficulty\",\n",
    "            type=\"str\",\n",
    "            description=\"Difficulty level: 'beginner', 'intermediate', or 'advanced'\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"year\",\n",
    "            type=\"int\",\n",
    "            description=\"Year of publication (2023 or 2024)\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Create auto-retriever\n",
    "auto_retriever = VectorIndexAutoRetriever(\n",
    "    chroma_index,\n",
    "    vector_store_info=vector_store_info,\n",
    "    similarity_top_k=3,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ VectorIndexAutoRetriever configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Tell me about beginner-level topics\n",
      "\n",
      "Auto-retriever will automatically extract metadata filters from the query!\n",
      "\n",
      "Retrieved 2 nodes:\n",
      "\n",
      "Node 1:\n",
      "  Topic: chroma\n",
      "  Difficulty: beginner\n",
      "  Score: 0.2776\n",
      "\n",
      "Node 2:\n",
      "  Topic: embeddings\n",
      "  Difficulty: beginner\n",
      "  Score: 0.2651\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query with natural language filters\n",
    "query_with_filter = \"Tell me about beginner-level topics\"\n",
    "\n",
    "print(f\"Query: {query_with_filter}\\n\")\n",
    "print(\"Auto-retriever will automatically extract metadata filters from the query!\\n\")\n",
    "\n",
    "retrieved = auto_retriever.retrieve(query_with_filter)\n",
    "\n",
    "print(f\"Retrieved {len(retrieved)} nodes:\\n\")\n",
    "for i, node in enumerate(retrieved, 1):\n",
    "    print(f\"Node {i}:\")\n",
    "    print(f\"  Topic: {node.metadata.get('topic')}\")\n",
    "    print(f\"  Difficulty: {node.metadata.get('difficulty')}\")\n",
    "    print(f\"  Score: {node.score:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto-Retriever Advantages\n",
    "\n",
    "**How it works:**\n",
    "1. LLM extracts metadata filters from natural language query\n",
    "2. Applies filters to vector store\n",
    "3. Performs similarity search on filtered subset\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Natural language interface to metadata\n",
    "- ‚úÖ No manual filter construction\n",
    "- ‚úÖ User-friendly for non-technical users\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå Extra LLM call (cost + latency)\n",
    "- ‚ùå May not extract complex filters correctly\n",
    "- ‚ùå Requires well-defined metadata schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Summary: What You Learned\n",
    "\n",
    "### ‚úÖ Completed Learning Objectives\n",
    "\n",
    "1. **Vector Store Integration**: Used SimpleVectorStore, Chroma, and Qdrant\n",
    "2. **Embedding Comparison**: Tested OpenAI vs HuggingFace embeddings\n",
    "3. **Index Persistence**: Saved and loaded indexes from disk\n",
    "4. **Query Engines**: Configured different response modes and streaming\n",
    "5. **Retrievers**: Implemented VectorIndexRetriever and VectorIndexAutoRetriever\n",
    "6. **Production Patterns**: Learned trade-offs for real-world deployment\n",
    "\n",
    "### Key Concepts Mastered\n",
    "\n",
    "- **Vector databases**: Chroma (easy), Qdrant (production), SimpleVectorStore (dev)\n",
    "- **Embedding models**: OpenAI (high quality, API), HuggingFace (free, local)\n",
    "- **Response modes**: compact, tree_summarize, refine, accumulate\n",
    "- **Streaming**: Better UX for user-facing applications\n",
    "- **Custom retrievers**: Direct control over retrieval logic\n",
    "- **Auto-retriever**: Natural language metadata filtering\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Practice Exercises\n",
    "\n",
    "1. **Vector DB Comparison**: Create the same index in SimpleVectorStore, Chroma, and Qdrant. Compare query speeds.\n",
    "2. **Embedding Experiment**: Build two indexes with different embeddings (OpenAI vs HuggingFace). Which retrieves better?\n",
    "3. **Response Modes**: Test all response modes on a complex query. Which gives the best answer?\n",
    "4. **Persistence**: Create an index, persist it, load it in a new session, verify it works.\n",
    "5. **Auto-Retriever**: Add more metadata fields and test natural language filtering.\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- **Vector Stores**: https://docs.llamaindex.ai/en/stable/module_guides/storing/vector_stores/\n",
    "- **Embeddings**: https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings/\n",
    "- **Query Engines**: https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/\n",
    "- **Retrievers**: https://docs.llamaindex.ai/en/stable/module_guides/querying/retriever/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

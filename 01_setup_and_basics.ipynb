{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Setup & Basics\n",
    "\n",
    "**Difficulty:** Beginner | **Estimated Time:** 45-60 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. ‚úÖ Understand LlamaIndex architecture and modular ecosystem\n",
    "2. ‚úÖ Install and configure LlamaIndex with the latest modular packages\n",
    "3. ‚úÖ Configure the Settings object (LLM, embeddings, chunk size)\n",
    "4. ‚úÖ Create your first VectorStoreIndex from documents\n",
    "5. ‚úÖ Execute basic queries and analyze responses\n",
    "6. ‚úÖ Understand the Document ‚Üí Node ‚Üí Index flow\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python 3.12+ installed\n",
    "- Understanding of embeddings and vector similarity\n",
    "- OpenAI API key (get from https://platform.openai.com/)\n",
    "\n",
    "## Curriculum Coverage\n",
    "\n",
    "- **Section 1.1:** Introduction to LlamaIndex\n",
    "- **Section 1.2:** Installation and Configuration\n",
    "- **Section 1.3.1:** Index Types (overview)\n",
    "- **Section 1.3.2:** Document and Node Structure (basics)\n",
    "- **Section 1.4:** Global Settings and Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introduction to LlamaIndex\n",
    "\n",
    "### What is LlamaIndex?\n",
    "\n",
    "LlamaIndex is a **data framework** for LLM-based applications, specifically designed to:\n",
    "\n",
    "- **Ingest** data from various sources (PDFs, APIs, databases)\n",
    "- **Index** data into optimized structures for retrieval\n",
    "- **Query** data with natural language\n",
    "- **Integrate** with LLMs for context-aware responses\n",
    "\n",
    "### Why LlamaIndex Matters\n",
    "\n",
    "For ML engineers, LlamaIndex solves key challenges:\n",
    "\n",
    "1. **Context Window Limitations**: LLMs have token limits (~8k-128k). LlamaIndex enables querying unlimited documents.\n",
    "2. **Semantic Search**: Goes beyond keyword matching using embedding-based similarity.\n",
    "3. **Source Attribution**: Tracks which documents contribute to responses.\n",
    "4. **Production-Ready**: Modular architecture, extensive integrations, active development.\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "```\n",
    "Documents ‚Üí Nodes ‚Üí Index ‚Üí Query Engine ‚Üí LLM ‚Üí Response\n",
    "     ‚Üì         ‚Üì       ‚Üì          ‚Üì\n",
    "   Load     Chunk   Embed     Retrieve\n",
    "```\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "- **Documents**: Raw data sources (PDFs, text, APIs)\n",
    "- **Nodes**: Chunked text with metadata\n",
    "- **Embeddings**: Vector representations of nodes\n",
    "- **Index**: Optimized storage for retrieval (VectorStoreIndex, SummaryIndex, etc.)\n",
    "- **Query Engine**: Orchestrates retrieval and synthesis\n",
    "- **Response Synthesis**: Combines retrieved context with LLM generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Installation & Modular Architecture\n",
    "\n",
    "### New Modular Package Structure\n",
    "\n",
    "LlamaIndex has shifted to a **three-tier architecture**:\n",
    "\n",
    "1. **llama-index-core**: Base abstractions (no integrations)\n",
    "2. **Integration packages**: Specific LLMs, embeddings, vector stores\n",
    "   - `llama-index-llms-openai`\n",
    "   - `llama-index-embeddings-huggingface`\n",
    "   - `llama-index-vector-stores-qdrant`\n",
    "3. **llama-index (meta)**: Bundles core + default integrations\n",
    "\n",
    "### Why Modular?\n",
    "\n",
    "- **Cherry-pick** only what you need\n",
    "- **Independent versioning** for each integration\n",
    "- **Smaller dependencies** = faster installs\n",
    "- **Future-proof** with active development\n",
    "\n",
    "### Installation\n",
    "\n",
    "If you followed the README setup, dependencies are already installed from `requirements.txt`. If not:\n",
    "\n",
    "```bash\n",
    "pip install llama-index-core llama-index-llms-openai llama-index-embeddings-openai llama-index-readers-file\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Imports & API Key Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Core LlamaIndex\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings, Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# LLM Integration\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Embedding Integration\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# Utilities\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load API Keys from .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI API key loaded (starts with: sk-proj-...)\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify OpenAI API key is set\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\n",
    "        \"‚ùå OPENAI_API_KEY not found!\\n\"\n",
    "        \"Please create a .env file in the project root with:\\n\"\n",
    "        \"OPENAI_API_KEY=your_key_here\"\n",
    "    )\n",
    "\n",
    "print(f\"‚úÖ OpenAI API key loaded (starts with: {openai_api_key[:8]}...)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Global Settings Configuration\n",
    "\n",
    "### Understanding the Settings Object\n",
    "\n",
    "The `Settings` object is the **modern way** to configure LlamaIndex globally (replaces deprecated `ServiceContext`).\n",
    "\n",
    "**Key Configuration Options:**\n",
    "\n",
    "- `Settings.llm`: Default LLM for query engines\n",
    "- `Settings.embed_model`: Default embedding model\n",
    "- `Settings.chunk_size`: Default chunk size for text splitting\n",
    "- `Settings.chunk_overlap`: Overlap between chunks\n",
    "- `Settings.node_parser`: Default node parser\n",
    "\n",
    "### Why Configure Settings?\n",
    "\n",
    "- **Consistency**: All components use same LLM/embeddings\n",
    "- **Convenience**: No need to pass parameters repeatedly\n",
    "- **Best Practices**: Centralized configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Global Settings configured successfully!\n",
      "   LLM: gpt-4o-mini\n",
      "   Embedding: text-embedding-3-small\n",
      "   Chunk size: 1024 tokens\n",
      "   Chunk overlap: 200 tokens\n"
     ]
    }
   ],
   "source": [
    "# Configure LLM\n",
    "Settings.llm = OpenAI(\n",
    "    model=\"gpt-4o-mini\",  # Fast, cost-effective for most use cases\n",
    "    temperature=0.1,      # Low temperature for consistent responses\n",
    ")\n",
    "\n",
    "# Configure Embedding Model\n",
    "Settings.embed_model = OpenAIEmbedding(\n",
    "    model=\"text-embedding-3-small\",  # 1536 dimensions, good balance\n",
    "    dimensions=1536,                 # Can be reduced for speed (e.g., 512)\n",
    ")\n",
    "\n",
    "# Configure Text Chunking\n",
    "Settings.chunk_size = 1024           # Tokens per chunk (typical: 512-1024)\n",
    "Settings.chunk_overlap = 200         # 20% overlap helps preserve context\n",
    "\n",
    "# Configure Node Parser\n",
    "Settings.node_parser = SentenceSplitter(\n",
    "    chunk_size=Settings.chunk_size,\n",
    "    chunk_overlap=Settings.chunk_overlap,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Global Settings configured successfully!\")\n",
    "print(f\"   LLM: {Settings.llm.model}\")\n",
    "print(f\"   Embedding: {Settings.embed_model.model_name}\")\n",
    "print(f\"   Chunk size: {Settings.chunk_size} tokens\")\n",
    "print(f\"   Chunk overlap: {Settings.chunk_overlap} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ ML Engineering Note: Embedding Dimensions\n",
    "\n",
    "**text-embedding-3-small** supports variable dimensions:\n",
    "\n",
    "- **1536 (default)**: Best quality, slower, more storage\n",
    "- **512**: 50% faster, 67% less storage, minimal quality loss\n",
    "- **256**: 75% faster, 83% less storage, noticeable quality loss\n",
    "\n",
    "**Trade-off**: For this tutorial, we use 1536 for best quality. In production, benchmark with your data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Loading Your First Document\n",
    "\n",
    "### Creating Sample Data\n",
    "\n",
    "For this example, we'll create a simple text document about LlamaIndex. In practice, you'd load from PDFs, APIs, databases, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created 3 sample documents\n",
      "   Total characters: 1169\n"
     ]
    }
   ],
   "source": [
    "# Create sample documents (in practice, load from files)\n",
    "documents = [\n",
    "    Document(\n",
    "        text=\"\"\"\n",
    "        LlamaIndex is a data framework for large language models (LLMs). \n",
    "        It provides tools to ingest, structure, and access private or domain-specific data.\n",
    "        LlamaIndex was created to solve the problem of connecting LLMs to external data sources.\n",
    "        The framework supports various data sources including PDFs, databases, APIs, and web pages.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"intro\", \"category\": \"overview\"}\n",
    "    ),\n",
    "    Document(\n",
    "        text=\"\"\"\n",
    "        Vector embeddings are numerical representations of text that capture semantic meaning.\n",
    "        In LlamaIndex, embeddings enable semantic search - finding relevant content based on meaning,\n",
    "        not just keyword matching. The default embedding model is OpenAI's text-embedding-3-small,\n",
    "        which produces 1536-dimensional vectors. Other models like all-MiniLM-L6-v2 produce 384 dimensions.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"embeddings\", \"category\": \"technical\"}\n",
    "    ),\n",
    "    Document(\n",
    "        text=\"\"\"\n",
    "        The VectorStoreIndex is the most common index type in LlamaIndex. It stores document embeddings\n",
    "        in a vector database and performs similarity search during queries. When you query the index,\n",
    "        it retrieves the most semantically similar chunks and passes them to the LLM as context.\n",
    "        This is the foundation of Retrieval-Augmented Generation (RAG).\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"vector_index\", \"category\": \"technical\"}\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Created {len(documents)} sample documents\")\n",
    "print(f\"   Total characters: {sum(len(doc.text) for doc in documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Document Objects\n",
    "\n",
    "**Document** is the base container in LlamaIndex:\n",
    "\n",
    "```python\n",
    "Document(\n",
    "    text=\"...\",           # The actual content\n",
    "    metadata={...},       # Custom metadata (source, date, author, etc.)\n",
    "    doc_id=\"...\",        # Optional: explicit ID\n",
    ")\n",
    "```\n",
    "\n",
    "**Metadata** is crucial for:\n",
    "- Filtering during retrieval\n",
    "- Source attribution in responses\n",
    "- Provenance tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Creating Your First VectorStoreIndex\n",
    "\n",
    "### The Magic: from_documents()\n",
    "\n",
    "This single method handles:\n",
    "1. **Chunking**: Splits documents into nodes using `Settings.node_parser`\n",
    "2. **Embedding**: Generates vectors using `Settings.embed_model`\n",
    "3. **Indexing**: Stores in vector store (in-memory by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating VectorStoreIndex...\n",
      "This will:\n",
      "  1. Chunk documents into nodes\n",
      "  2. Generate embeddings for each node\n",
      "  3. Store in in-memory vector store\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34918f80606c47fe8523a5526ca34423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c8d164355a3414aac359c868c461c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Index created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create index from documents\n",
    "print(\"Creating VectorStoreIndex...\")\n",
    "print(\"This will:\")\n",
    "print(\"  1. Chunk documents into nodes\")\n",
    "print(\"  2. Generate embeddings for each node\")\n",
    "print(\"  3. Store in in-memory vector store\\n\")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    show_progress=True,  # Display progress bar\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Index created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "Behind the scenes:\n",
    "\n",
    "1. **Document ‚Üí Nodes**: Each document was split into smaller chunks (nodes)\n",
    "2. **Nodes ‚Üí Embeddings**: Each node was embedded using OpenAI's API\n",
    "3. **Embeddings ‚Üí Index**: Vectors were stored in `SimpleVectorStore` (in-memory)\n",
    "\n",
    "**Index Types** (we'll explore others in later notebooks):\n",
    "- `VectorStoreIndex`: Semantic similarity search (most common)\n",
    "- `SummaryIndex`: Sequential scanning (good for summaries)\n",
    "- `TreeIndex`: Hierarchical structure\n",
    "- `KeywordTableIndex`: Keyword extraction\n",
    "- `KnowledgeGraphIndex`: Entity relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Basic Querying\n",
    "\n",
    "### Creating a Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Query engine created!\n",
      "   Top-K: 2\n",
      "   Response mode: compact\n"
     ]
    }
   ],
   "source": [
    "# Create query engine from index\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=2,  # Retrieve top 2 most similar chunks\n",
    "    response_mode=\"compact\",  # Compact response synthesis\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Query engine created!\")\n",
    "print(f\"   Top-K: {2}\")\n",
    "print(f\"   Response mode: compact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Your First Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is LlamaIndex used for?\n",
      "\n",
      "Response:\n",
      "--------------------------------------------------------------------------------\n",
      "LlamaIndex is used as a data framework for large language models (LLMs) to ingest, structure, and access private or domain-specific data. It connects LLMs to various external data sources such as PDFs, databases, APIs, and web pages.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Query the index\n",
    "query = \"What is LlamaIndex used for?\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "response = query_engine.query(query)\n",
    "\n",
    "print(\"Response:\")\n",
    "print(\"-\" * 80)\n",
    "print(response)\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the Response\n",
    "\n",
    "Let's examine what was retrieved and how it was used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of source nodes: 2\n",
      "\n",
      "Source Node 1:\n",
      "  Score: 0.7579\n",
      "  Metadata: {'source': 'intro', 'category': 'overview'}\n",
      "  Text (first 200 chars): LlamaIndex is a data framework for large language models (LLMs). \n",
      "        It provides tools to ingest, structure, and access private or domain-specific data.\n",
      "        LlamaIndex was created to solve th...\n",
      "\n",
      "Source Node 2:\n",
      "  Score: 0.5726\n",
      "  Metadata: {'source': 'vector_index', 'category': 'technical'}\n",
      "  Text (first 200 chars): The VectorStoreIndex is the most common index type in LlamaIndex. It stores document embeddings\n",
      "        in a vector database and performs similarity search during queries. When you query the index,\n",
      "  ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect source nodes\n",
    "print(f\"Number of source nodes: {len(response.source_nodes)}\\n\")\n",
    "\n",
    "for i, node in enumerate(response.source_nodes, 1):\n",
    "    print(f\"Source Node {i}:\")\n",
    "    print(f\"  Score: {node.score:.4f}\")  # Similarity score (0-1)\n",
    "    print(f\"  Metadata: {node.metadata}\")\n",
    "    print(f\"  Text (first 200 chars): {node.text[:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ ML Engineering Note: Similarity Scores\n",
    "\n",
    "**Score Interpretation** (cosine similarity):\n",
    "- **1.0**: Perfect match (identical vectors)\n",
    "- **0.9-1.0**: Highly relevant\n",
    "- **0.7-0.9**: Relevant\n",
    "- **0.5-0.7**: Somewhat relevant\n",
    "- **< 0.5**: Likely not relevant\n",
    "\n",
    "**Why this matters**: You can set similarity thresholds to filter out low-quality retrievals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Experimenting with Different Queries\n",
    "\n",
    "### Query 1: Embedding-Specific Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How do embeddings work in LlamaIndex?\n",
      "\n",
      "Response:\n",
      "Embeddings in LlamaIndex function as numerical representations of text that capture semantic meaning. They facilitate semantic search, allowing users to find relevant content based on meaning rather than relying solely on keyword matching. The framework utilizes a default embedding model, which generates 1536-dimensional vectors, while alternative models can produce vectors with fewer dimensions, such as 384. This capability enhances the connection between large language models and external data sources.\n",
      "\n",
      "Top retrieved source:\n",
      "  Category: technical\n",
      "  Score: 0.7120\n"
     ]
    }
   ],
   "source": [
    "query1 = \"How do embeddings work in LlamaIndex?\"\n",
    "response1 = query_engine.query(query1)\n",
    "\n",
    "print(f\"Query: {query1}\\n\")\n",
    "print(\"Response:\")\n",
    "print(response1)\n",
    "print(\"\\nTop retrieved source:\")\n",
    "print(f\"  Category: {response1.source_nodes[0].metadata.get('category')}\")\n",
    "print(f\"  Score: {response1.source_nodes[0].score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 2: RAG-Specific Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is Retrieval-Augmented Generation?\n",
      "\n",
      "Response:\n",
      "Retrieval-Augmented Generation (RAG) is a process that involves retrieving semantically similar chunks of information from a vector database and using them as context for generating responses. This approach enhances the generation of text by incorporating relevant information based on meaning rather than just keywords.\n"
     ]
    }
   ],
   "source": [
    "query2 = \"What is Retrieval-Augmented Generation?\"\n",
    "response2 = query_engine.query(query2)\n",
    "\n",
    "print(f\"Query: {query2}\\n\")\n",
    "print(\"Response:\")\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Understanding the Document ‚Üí Node ‚Üí Index Flow\n",
    "\n",
    "### Inspecting Nodes Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes created: 3\n",
      "\n",
      "Node 1:\n",
      "  ID: fc9acd1e-5da7-4216-9e0e-b48877c19812\n",
      "  Text length: 354 characters\n",
      "  Metadata: {'source': 'intro', 'category': 'overview'}\n",
      "  Relationships: {<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b565cf69-bc9f-49e5-a398-a8e3c1275d0a', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'source': 'intro', 'category': 'overview'}, hash='6b67115e521a90d22245235f7a97b2808451a01b637bc146fc4c6b1b0126d392')}\n",
      "\n",
      "Node 2:\n",
      "  ID: 7d63512c-ce44-41da-b632-a3cd076a7e5f\n",
      "  Text length: 395 characters\n",
      "  Metadata: {'source': 'embeddings', 'category': 'technical'}\n",
      "  Relationships: {<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='5b0c8e3e-de61-4805-a087-12679c0c674d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'source': 'embeddings', 'category': 'technical'}, hash='c74e00722500b0b510e568d1fd63750c229430501577b8d0a9295eaa48b99fb3')}\n",
      "\n",
      "Node 3:\n",
      "  ID: 5e1995cb-eac9-4076-a302-556d411caa18\n",
      "  Text length: 366 characters\n",
      "  Metadata: {'source': 'vector_index', 'category': 'technical'}\n",
      "  Relationships: {<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='3dcfd692-d623-4ce5-aa4e-c1223da88f12', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'source': 'vector_index', 'category': 'technical'}, hash='e70f293d1f0d5f862e85b3f38f9fcdd444e6238533b3c87fb9d78272b4a683bd')}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parse documents into nodes manually to understand the flow\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "parser = SentenceSplitter(chunk_size=1024, chunk_overlap=200)\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "print(f\"Number of nodes created: {len(nodes)}\\n\")\n",
    "\n",
    "for i, node in enumerate(nodes, 1):\n",
    "    print(f\"Node {i}:\")\n",
    "    print(f\"  ID: {node.node_id}\")\n",
    "    print(f\"  Text length: {len(node.text)} characters\")\n",
    "    print(f\"  Metadata: {node.metadata}\")\n",
    "    print(f\"  Relationships: {node.relationships}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Relationships\n",
    "\n",
    "Nodes track relationships:\n",
    "- **SOURCE**: Link to original document\n",
    "- **PREVIOUS/NEXT**: Sequential order\n",
    "- **PARENT/CHILD**: Hierarchical structure\n",
    "\n",
    "This enables advanced retrieval (covered in Notebook 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Adjusting similarity_top_k\n",
    "\n",
    "### Impact of Top-K on Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-K = 1:\n",
      "  Retrieved 1 nodes\n",
      "  Response length: 395 characters\n",
      "  First source score: 0.5801\n",
      "\n",
      "Top-K = 2:\n",
      "  Retrieved 2 nodes\n",
      "  Response length: 444 characters\n",
      "  First source score: 0.5801\n",
      "\n",
      "Top-K = 3:\n",
      "  Retrieved 3 nodes\n",
      "  Response length: 433 characters\n",
      "  First source score: 0.5801\n"
     ]
    }
   ],
   "source": [
    "# Test with different top_k values\n",
    "test_query = \"Explain vector embeddings\"\n",
    "\n",
    "for k in [1, 2, 3]:\n",
    "    engine = index.as_query_engine(similarity_top_k=k)\n",
    "    response = engine.query(test_query)\n",
    "    \n",
    "    print(f\"\\nTop-K = {k}:\")\n",
    "    print(f\"  Retrieved {len(response.source_nodes)} nodes\")\n",
    "    print(f\"  Response length: {len(str(response))} characters\")\n",
    "    print(f\"  First source score: {response.source_nodes[0].score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ ML Engineering Note: Choosing Top-K\n",
    "\n",
    "**Trade-offs**:\n",
    "\n",
    "- **Low K (1-2)**:\n",
    "  - ‚úÖ Faster queries\n",
    "  - ‚úÖ Lower LLM costs (fewer tokens)\n",
    "  - ‚ùå May miss relevant context\n",
    "\n",
    "- **Medium K (3-5)**:\n",
    "  - ‚úÖ Balanced retrieval\n",
    "  - ‚úÖ Good default for most use cases\n",
    "  - ‚ö†Ô∏è Moderate cost/speed\n",
    "\n",
    "- **High K (10+)**:\n",
    "  - ‚úÖ Comprehensive context\n",
    "  - ‚ùå Slower queries\n",
    "  - ‚ùå Higher LLM costs\n",
    "  - ‚ùå Risk of context dilution\n",
    "\n",
    "**Best Practice**: Start with k=3, tune based on your data and query complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Response Modes\n",
    "\n",
    "LlamaIndex supports different **response synthesis** strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mode: compact\n",
      "Response: LlamaIndex is a data framework designed for large language models, offering tools to ingest, structure, and access private or domain-specific data. It connects LLMs to various external data sources, including PDFs, databases, APIs, and web pages. A key feature is the VectorStoreIndex, which stores document embeddings in a vector database and enables similarity searches during queries, facilitating the retrieval of semantically similar chunks to provide context for the LLM. This functionality underpins Retrieval-Augmented Generation (RAG).\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Mode: tree_summarize\n",
      "Response: LlamaIndex is a data framework designed for large language models, offering tools to ingest, structure, and access private or domain-specific data. It connects LLMs to various external data sources, including PDFs, databases, APIs, and web pages. A key feature is the VectorStoreIndex, which stores document embeddings in a vector database and enables similarity searches during queries. This allows for the retrieval of semantically similar chunks of information, which are then provided to the LLM as context, forming the basis of Retrieval-Augmented Generation (RAG).\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Mode: simple_summarize\n",
      "Response: LlamaIndex is a data framework designed for large language models, offering tools to ingest, structure, and access private or domain-specific data. It connects LLMs to various external data sources, including PDFs, databases, APIs, and web pages. A key feature is the VectorStoreIndex, which stores document embeddings in a vector database and enables similarity searches during queries, facilitating the retrieval of semantically similar chunks to provide context for the LLM. This functionality underpins Retrieval-Augmented Generation (RAG).\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test different response modes\n",
    "modes = [\"compact\", \"tree_summarize\", \"simple_summarize\"]\n",
    "test_query = \"What are the key features of LlamaIndex?\"\n",
    "\n",
    "for mode in modes:\n",
    "    engine = index.as_query_engine(\n",
    "        similarity_top_k=2,\n",
    "        response_mode=mode\n",
    "    )\n",
    "    response = engine.query(test_query)\n",
    "    \n",
    "    print(f\"\\nMode: {mode}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Mode Comparison\n",
    "\n",
    "| Mode | How It Works | Best For |\n",
    "|------|-------------|----------|\n",
    "| **compact** | Concatenates chunks, refines iteratively | Balanced quality/speed |\n",
    "| **tree_summarize** | Builds summary tree hierarchically | Large context, comprehensive answers |\n",
    "| **simple_summarize** | Concatenates all chunks, single LLM call | Simple queries, speed |\n",
    "| **refine** | Iteratively refines answer with each chunk | High quality, slower |\n",
    "| **accumulate** | Generates separate answer per chunk | Multiple perspectives |\n",
    "\n",
    "**Default**: `compact` (good balance for most use cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Summary: What You Learned\n",
    "\n",
    "### ‚úÖ Completed Learning Objectives\n",
    "\n",
    "1. **LlamaIndex Architecture**: Understood the modular package structure and data flow\n",
    "2. **Installation**: Set up latest modular packages (llama-index-core + integrations)\n",
    "3. **Settings Configuration**: Configured global LLM, embeddings, and chunking parameters\n",
    "4. **VectorStoreIndex**: Created your first index using `from_documents()`\n",
    "5. **Query Execution**: Executed queries and analyzed responses with source attribution\n",
    "6. **Document Flow**: Understood Document ‚Üí Node ‚Üí Embedding ‚Üí Index ‚Üí Query pipeline\n",
    "\n",
    "### Key Concepts Mastered\n",
    "\n",
    "- **Settings object** replaces deprecated ServiceContext\n",
    "- **VectorStoreIndex** is the most common index type\n",
    "- **similarity_top_k** controls retrieval breadth\n",
    "- **Response modes** affect synthesis strategy\n",
    "- **Source nodes** enable provenance tracking\n",
    "- **Cosine similarity** scores indicate relevance (0-1)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In **Notebook 2: Documents & Chunking**, you'll learn:\n",
    "- Loading documents from multiple sources (PDFs, web, databases)\n",
    "- Advanced chunking strategies (sentence, token, semantic)\n",
    "- Metadata management and filtering\n",
    "- Node relationships and hierarchies\n",
    "- Optimizing chunk size for your use case\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Practice Exercises\n",
    "\n",
    "1. **Experiment with chunk sizes**: Try `chunk_size=512` vs `chunk_size=2048`. How does it affect the number of nodes and query responses?\n",
    "\n",
    "2. **Test with your own data**: Replace the sample documents with a PDF or text file from `data/sample_docs/`\n",
    "\n",
    "3. **Tune top_k**: Query the same question with k=1, k=3, k=5. Compare response quality and source coverage.\n",
    "\n",
    "4. **Embedding dimensions**: Change `OpenAIEmbedding(dimensions=512)` and observe speed differences.\n",
    "\n",
    "5. **Response modes**: Test all response modes on a complex query. Which provides the best answer?\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- **LlamaIndex Docs**: https://docs.llamaindex.ai/en/stable/\n",
    "- **Settings API**: https://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/settings/\n",
    "- **VectorStoreIndex**: https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index/\n",
    "- **OpenAI Embeddings**: https://platform.openai.com/docs/guides/embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

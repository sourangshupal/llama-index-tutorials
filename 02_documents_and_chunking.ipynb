{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Documents & Chunking\n",
    "\n",
    "**Difficulty:** Beginner-Intermediate | **Estimated Time:** 90-120 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. ‚úÖ Load documents from multiple sources (local files, PDFs, web)\n",
    "2. ‚úÖ Implement different chunking strategies (sentence, token, semantic)\n",
    "3. ‚úÖ Add custom metadata at document and node levels\n",
    "4. ‚úÖ Create and manage node relationships\n",
    "5. ‚úÖ Optimize chunking for retrieval quality\n",
    "6. ‚úÖ Apply batch embedding optimization\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Notebook 1: Setup & Basics\n",
    "- Understanding of embeddings and chunking concepts\n",
    "- Sample PDFs in `data/research_papers/` directory\n",
    "\n",
    "## Curriculum Coverage\n",
    "\n",
    "- **Section 2.1:** Loading Documents from Various Sources\n",
    "- **Section 2.2:** Document Preprocessing\n",
    "- **Section 2.3:** Document Parsing and Chunking\n",
    "- **Section 2.4:** Metadata Management\n",
    "- **Section 2.5:** Document Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Core LlamaIndex\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings, Document\n",
    "from llama_index.core.schema import TextNode, NodeRelationship, RelatedNodeInfo\n",
    "\n",
    "# Node Parsers (Chunking Strategies)\n",
    "from llama_index.core.node_parser import (\n",
    "    SentenceSplitter,\n",
    "    TokenTextSplitter,\n",
    "    SemanticSplitterNodeParser,\n",
    ")\n",
    "\n",
    "# Metadata Extraction\n",
    "from llama_index.core.extractors import (\n",
    "    TitleExtractor,\n",
    "    SummaryExtractor,\n",
    ")\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "# LLM and Embeddings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# Utilities\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Settings configured\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables and configure Settings\n",
    "load_dotenv()\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "Settings.embed_model = OpenAIEmbedding(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    dimensions=1536\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Settings configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Loading Documents from Multiple Sources\n",
    "\n",
    "### 2.1 Local File Loading with SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory exists: True\n",
      "Sample docs directory: True\n",
      "Research papers directory: True\n",
      "\n",
      "Found 2 PDF files in research_papers/\n",
      "  - brain_tumor_2024_removed.pdf\n",
      "  - brain_tumor_2023_2_removed.pdf\n"
     ]
    }
   ],
   "source": [
    "# Check data directory structure\n",
    "data_dir = Path(\"./data\")\n",
    "sample_docs_dir = data_dir / \"sample_docs\"\n",
    "research_papers_dir = data_dir / \"research_papers\"\n",
    "\n",
    "print(f\"Data directory exists: {data_dir.exists()}\")\n",
    "print(f\"Sample docs directory: {sample_docs_dir.exists()}\")\n",
    "print(f\"Research papers directory: {research_papers_dir.exists()}\")\n",
    "\n",
    "if research_papers_dir.exists():\n",
    "    files = list(research_papers_dir.glob(\"*.pdf\"))\n",
    "    print(f\"\\nFound {len(files)} PDF files in research_papers/\")\n",
    "    for f in files:\n",
    "        print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleDirectoryReader Features\n",
    "\n",
    "**Key Parameters:**\n",
    "- `input_dir`: Directory path\n",
    "- `required_exts`: Filter by extensions (e.g., `[\".pdf\", \".txt\"]`)\n",
    "- `recursive`: Scan subdirectories\n",
    "- `filename_as_id`: Use filename as document ID\n",
    "- `file_metadata`: Custom metadata function\n",
    "- `exclude_hidden`: Skip hidden files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created 3 sample research papers\n",
      "  - Attention Is All You Need (2017)\n",
      "  - BERT (2019)\n",
      "  - RAG (2020)\n"
     ]
    }
   ],
   "source": [
    "# Create sample documents if no PDFs available\n",
    "# In practice, you'd load actual PDFs from the data directory\n",
    "\n",
    "sample_papers = [\n",
    "    Document(\n",
    "        text=\"\"\"\n",
    "        Title: Attention Is All You Need\n",
    "        Authors: Vaswani et al.\n",
    "        Year: 2017\n",
    "        \n",
    "        Abstract: The dominant sequence transduction models are based on complex recurrent or \n",
    "        convolutional neural networks that include an encoder and a decoder. The best performing \n",
    "        models also connect the encoder and decoder through an attention mechanism. We propose a \n",
    "        new simple network architecture, the Transformer, based solely on attention mechanisms, \n",
    "        dispensing with recurrence and convolutions entirely.\n",
    "        \n",
    "        Introduction: Recurrent neural networks, long short-term memory and gated recurrent neural \n",
    "        networks in particular, have been firmly established as state of the art approaches in \n",
    "        sequence modeling and transduction problems. The Transformer is the first transduction model \n",
    "        relying entirely on self-attention to compute representations of its input and output without \n",
    "        using sequence-aligned RNNs or convolution.\n",
    "        \"\"\",\n",
    "        metadata={\n",
    "            \"title\": \"Attention Is All You Need\",\n",
    "            \"authors\": \"Vaswani et al.\",\n",
    "            \"year\": 2017,\n",
    "            \"category\": \"transformers\",\n",
    "            \"citations\": 85000,\n",
    "            \"source\": \"research_paper\"\n",
    "        }\n",
    "    ),\n",
    "    Document(\n",
    "        text=\"\"\"\n",
    "        Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
    "        Authors: Devlin et al.\n",
    "        Year: 2019\n",
    "        \n",
    "        Abstract: We introduce a new language representation model called BERT, which stands for \n",
    "        Bidirectional Encoder Representations from Transformers. Unlike recent language representation \n",
    "        models, BERT is designed to pre-train deep bidirectional representations from unlabeled text \n",
    "        by jointly conditioning on both left and right context in all layers.\n",
    "        \n",
    "        Introduction: Language model pre-training has been shown to be effective for improving many \n",
    "        natural language processing tasks. Pre-trained language representations can be either context-free \n",
    "        or context-based. BERT alleviates the unidirectionality constraint by using a masked language \n",
    "        model (MLM) pre-training objective.\n",
    "        \"\"\",\n",
    "        metadata={\n",
    "            \"title\": \"BERT\",\n",
    "            \"authors\": \"Devlin et al.\",\n",
    "            \"year\": 2019,\n",
    "            \"category\": \"language_models\",\n",
    "            \"citations\": 65000,\n",
    "            \"source\": \"research_paper\"\n",
    "        }\n",
    "    ),\n",
    "    Document(\n",
    "        text=\"\"\"\n",
    "        Title: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
    "        Authors: Lewis et al.\n",
    "        Year: 2020\n",
    "        \n",
    "        Abstract: Large pre-trained language models have been shown to store factual knowledge in their \n",
    "        parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, \n",
    "        their ability to access and precisely manipulate knowledge is still limited. We explore a general \n",
    "        fine-tuning recipe for retrieval-augmented generation (RAG) models which combine parametric and \n",
    "        non-parametric memory.\n",
    "        \n",
    "        Introduction: Pre-trained neural language models store and retrieve knowledge using their parameters. \n",
    "        RAG models combine parametric memory (the LLM) with non-parametric memory (a dense vector index of \n",
    "        Wikipedia). This provides the model with access to up-to-date information and allows for more \n",
    "        interpretable and modular systems.\n",
    "        \"\"\",\n",
    "        metadata={\n",
    "            \"title\": \"RAG\",\n",
    "            \"authors\": \"Lewis et al.\",\n",
    "            \"year\": 2020,\n",
    "            \"category\": \"rag\",\n",
    "            \"citations\": 3500,\n",
    "            \"source\": \"research_paper\"\n",
    "        }\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Created {len(sample_papers)} sample research papers\")\n",
    "for doc in sample_papers:\n",
    "    print(f\"  - {doc.metadata['title']} ({doc.metadata['year']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Custom Metadata Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced metadata for first document:\n",
      "  title: Attention Is All You Need\n",
      "  authors: Vaswani et al.\n",
      "  year: 2017\n",
      "  category: transformers\n",
      "  citations: 85000\n",
      "  source: research_paper\n",
      "  processed_date: 2025-12-21T07:43:02.600762\n",
      "  char_count: 1006\n",
      "  word_count: 123\n"
     ]
    }
   ],
   "source": [
    "# Add processing metadata\n",
    "for doc in sample_papers:\n",
    "    doc.metadata[\"processed_date\"] = datetime.now().isoformat()\n",
    "    doc.metadata[\"char_count\"] = len(doc.text)\n",
    "    doc.metadata[\"word_count\"] = len(doc.text.split())\n",
    "\n",
    "print(\"Enhanced metadata for first document:\")\n",
    "for key, value in sample_papers[0].metadata.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Chunking Strategies\n",
    "\n",
    "### Why Chunking Matters\n",
    "\n",
    "Chunking is **critical** for RAG quality:\n",
    "\n",
    "1. **Context Window Limits**: LLMs have token limits\n",
    "2. **Embedding Quality**: Smaller chunks = more focused embeddings\n",
    "3. **Retrieval Precision**: Granular chunks improve relevance\n",
    "4. **Cost Optimization**: Smaller chunks = fewer tokens to LLM\n",
    "\n",
    "### 3.1 Sentence-Based Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceSplitter Results:\n",
      "  Input documents: 3\n",
      "  Output nodes: 3\n",
      "  Avg chars per node: 936\n",
      "\n",
      "First node preview:\n",
      "  Text (first 200 chars): Title: Attention Is All You Need\n",
      "        Authors: Vaswani et al.\n",
      "        Year: 2017\n",
      "\n",
      "        Abstract: The dominant sequence transduction models are based on complex recurrent or \n",
      "        convolutiona...\n",
      "  Metadata: {'title': 'Attention Is All You Need', 'authors': 'Vaswani et al.', 'year': 2017, 'category': 'transformers', 'citations': 85000, 'source': 'research_paper', 'processed_date': '2025-12-21T07:43:02.600762', 'char_count': 1006, 'word_count': 123}\n"
     ]
    }
   ],
   "source": [
    "# SentenceSplitter: Respects sentence boundaries\n",
    "sentence_splitter = SentenceSplitter(\n",
    "    chunk_size=1024,     # Target tokens per chunk\n",
    "    chunk_overlap=200,   # Overlap to preserve context\n",
    "    separator=\" \",       # Split on spaces first\n",
    ")\n",
    "\n",
    "sentence_nodes = sentence_splitter.get_nodes_from_documents(sample_papers)\n",
    "\n",
    "print(f\"SentenceSplitter Results:\")\n",
    "print(f\"  Input documents: {len(sample_papers)}\")\n",
    "print(f\"  Output nodes: {len(sentence_nodes)}\")\n",
    "print(f\"  Avg chars per node: {sum(len(n.text) for n in sentence_nodes) / len(sentence_nodes):.0f}\")\n",
    "\n",
    "print(f\"\\nFirst node preview:\")\n",
    "print(f\"  Text (first 200 chars): {sentence_nodes[0].text[:200]}...\")\n",
    "print(f\"  Metadata: {sentence_nodes[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ ML Engineering Note: Chunk Size Selection\n",
    "\n",
    "**Chunk Size Trade-offs:**\n",
    "\n",
    "| Size | Pros | Cons | Use Case |\n",
    "|------|------|------|----------|\n",
    "| **Small (256-512)** | Precise retrieval, lower cost | May lose context | Q&A, factoid extraction |\n",
    "| **Medium (512-1024)** | Balanced context/precision | Good default | General RAG, document QA |\n",
    "| **Large (1024-2048)** | Rich context | Diluted relevance, higher cost | Summarization, broad queries |\n",
    "\n",
    "**Overlap Guidelines:**\n",
    "- 10-20% of chunk size (typical)\n",
    "- Higher overlap (20-30%) for dense, technical content\n",
    "- Lower overlap (5-10%) for structured documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Token-Based Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenTextSplitter Results:\n",
      "  Input documents: 3\n",
      "  Output nodes: 3\n",
      "  Avg chars per node: 936\n",
      "\n",
      "Comparison:\n",
      "  SentenceSplitter: 3 nodes\n",
      "  TokenTextSplitter: 3 nodes\n",
      "  Difference: 0 nodes\n"
     ]
    }
   ],
   "source": [
    "# TokenTextSplitter: Precise token count control\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=512,      # Exact token limit\n",
    "    chunk_overlap=128,   # 25% overlap\n",
    "    separator=\" \",\n",
    ")\n",
    "\n",
    "token_nodes = token_splitter.get_nodes_from_documents(sample_papers)\n",
    "\n",
    "print(f\"TokenTextSplitter Results:\")\n",
    "print(f\"  Input documents: {len(sample_papers)}\")\n",
    "print(f\"  Output nodes: {len(token_nodes)}\")\n",
    "print(f\"  Avg chars per node: {sum(len(n.text) for n in token_nodes) / len(token_nodes):.0f}\")\n",
    "\n",
    "# Compare with sentence splitter\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  SentenceSplitter: {len(sentence_nodes)} nodes\")\n",
    "print(f\"  TokenTextSplitter: {len(token_nodes)} nodes\")\n",
    "print(f\"  Difference: {abs(len(sentence_nodes) - len(token_nodes))} nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating semantic chunks (this will call embedding API)...\n",
      "\n",
      "SemanticSplitterNodeParser Results:\n",
      "  Input documents: 3\n",
      "  Output nodes: 6\n",
      "  Avg chars per node: 477\n",
      "  Min chars: 220\n",
      "  Max chars: 740\n"
     ]
    }
   ],
   "source": [
    "# SemanticSplitterNodeParser: Chunk by meaning, not just size\n",
    "semantic_splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1,              # Sentences to group for comparison\n",
    "    breakpoint_percentile_threshold=95,  # Sensitivity to semantic breaks\n",
    "    embed_model=Settings.embed_model,\n",
    ")\n",
    "\n",
    "print(\"Creating semantic chunks (this will call embedding API)...\")\n",
    "semantic_nodes = semantic_splitter.get_nodes_from_documents(sample_papers)\n",
    "\n",
    "print(f\"\\nSemanticSplitterNodeParser Results:\")\n",
    "print(f\"  Input documents: {len(sample_papers)}\")\n",
    "print(f\"  Output nodes: {len(semantic_nodes)}\")\n",
    "print(f\"  Avg chars per node: {sum(len(n.text) for n in semantic_nodes) / len(semantic_nodes):.0f}\")\n",
    "print(f\"  Min chars: {min(len(n.text) for n in semantic_nodes)}\")\n",
    "print(f\"  Max chars: {max(len(n.text) for n in semantic_nodes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Chunking Advantages\n",
    "\n",
    "**How it works:**\n",
    "1. Embeds consecutive sentences\n",
    "2. Calculates cosine similarity between embeddings\n",
    "3. Splits where similarity drops (topic change)\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Preserves semantic coherence\n",
    "- ‚úÖ Natural topic boundaries\n",
    "- ‚úÖ Better for complex documents\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå Slower (requires embedding API calls)\n",
    "- ‚ùå Variable chunk sizes\n",
    "- ‚ùå Higher cost (more API calls)\n",
    "\n",
    "**Best for**: Academic papers, technical docs, long-form content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Comparing Chunking Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunking Strategy Comparison:\n",
      "Strategy  Num Nodes  Avg Chars  Min Chars  Max Chars  Std Dev\n",
      "Sentence          3        936        877        988       55\n",
      "   Token          3        936        877        988       55\n",
      "Semantic          6        477        220        740      228\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Compare chunking strategies\n",
    "strategies = [\n",
    "    {\"name\": \"Sentence\", \"nodes\": sentence_nodes},\n",
    "    {\"name\": \"Token\", \"nodes\": token_nodes},\n",
    "    {\"name\": \"Semantic\", \"nodes\": semantic_nodes},\n",
    "]\n",
    "\n",
    "comparison_data = []\n",
    "for strat in strategies:\n",
    "    nodes = strat[\"nodes\"]\n",
    "    comparison_data.append({\n",
    "        \"Strategy\": strat[\"name\"],\n",
    "        \"Num Nodes\": len(nodes),\n",
    "        \"Avg Chars\": int(sum(len(n.text) for n in nodes) / len(nodes)),\n",
    "        \"Min Chars\": min(len(n.text) for n in nodes),\n",
    "        \"Max Chars\": max(len(n.text) for n in nodes),\n",
    "        \"Std Dev\": int(pd.Series([len(n.text) for n in nodes]).std()),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nChunking Strategy Comparison:\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Metadata Management\n",
    "\n",
    "### 4.1 Adding Custom Node-Level Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced node metadata example:\n",
      "Node 0 metadata: {'title': 'Attention Is All You Need', 'authors': 'Vaswani et al.', 'year': 2017, 'category': 'transformers', 'citations': 85000, 'source': 'research_paper', 'processed_date': '2025-12-21T07:43:02.600762', 'char_count': 1006, 'word_count': 123, 'node_index': 0, 'chunk_strategy': 'sentence', 'has_abstract': True, 'has_introduction': True, 'mentions_transformer': True}\n"
     ]
    }
   ],
   "source": [
    "# Enrich nodes with custom metadata\n",
    "for i, node in enumerate(sentence_nodes):\n",
    "    # Add node-specific metadata\n",
    "    node.metadata[\"node_index\"] = i\n",
    "    node.metadata[\"chunk_strategy\"] = \"sentence\"\n",
    "    \n",
    "    # Derive metadata from content\n",
    "    text_lower = node.text.lower()\n",
    "    node.metadata[\"has_abstract\"] = \"abstract\" in text_lower\n",
    "    node.metadata[\"has_introduction\"] = \"introduction\" in text_lower\n",
    "    node.metadata[\"mentions_transformer\"] = \"transformer\" in text_lower\n",
    "\n",
    "print(\"Enhanced node metadata example:\")\n",
    "print(f\"Node 0 metadata: {sentence_nodes[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 LLM-Based Metadata Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting metadata with LLM (this may take a moment)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Extracted summaries for 2 nodes\n",
      "\n",
      "Node 0 with LLM-generated summary:\n",
      "  Original text (first 150 chars): Title: Attention Is All You Need\n",
      "        Authors: Vaswani et al.\n",
      "        Year: 2017\n",
      "\n",
      "        Abstract: The dominant sequence transduction models are b...\n",
      "  Summary: The section discusses the research paper titled \"Attention Is All You Need,\" authored by Vaswani et al. in 2017. It introduces the Transformer architecture, which is a novel approach to sequence transduction that relies entirely on attention mechanisms, eliminating the need for recurrent or convolutional neural networks. The paper highlights the limitations of traditional models, such as recurrent neural networks (RNNs) and long short-term memory (LSTM) networks, in sequence modeling. The Transformer model is presented as a state-of-the-art solution that utilizes self-attention to process input and output representations effectively. The paper has garnered significant attention, with approximately 85,000 citations, indicating its impact on the field of machine learning and natural language processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Use LLM to extract metadata\n",
    "from llama_index.core.extractors import SummaryExtractor, TitleExtractor\n",
    "\n",
    "# Create extractors\n",
    "title_extractor = TitleExtractor(\n",
    "    llm=Settings.llm,\n",
    "    nodes=5,  # Look at first 5 nodes for title\n",
    ")\n",
    "\n",
    "summary_extractor = SummaryExtractor(\n",
    "    llm=Settings.llm,\n",
    "    summaries=[\"self\"],  # Summarize each node\n",
    ")\n",
    "\n",
    "print(\"Extracting metadata with LLM (this may take a moment)...\")\n",
    "\n",
    "# Apply to a subset of nodes (to save API calls)\n",
    "sample_nodes_for_extraction = sentence_nodes[:2]\n",
    "\n",
    "# Extract summaries\n",
    "nodes_with_summaries = summary_extractor.process_nodes(sample_nodes_for_extraction)\n",
    "\n",
    "print(f\"\\n‚úÖ Extracted summaries for {len(nodes_with_summaries)} nodes\")\n",
    "print(f\"\\nNode 0 with LLM-generated summary:\")\n",
    "print(f\"  Original text (first 150 chars): {nodes_with_summaries[0].text[:150]}...\")\n",
    "if \"section_summary\" in nodes_with_summaries[0].metadata:\n",
    "    print(f\"  Summary: {nodes_with_summaries[0].metadata['section_summary']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ ML Engineering Note: Metadata Extraction Trade-offs\n",
    "\n",
    "**LLM-based extraction:**\n",
    "- ‚úÖ High quality, contextual metadata\n",
    "- ‚úÖ Can extract complex information (topics, entities, sentiment)\n",
    "- ‚ùå Expensive (LLM API calls per node)\n",
    "- ‚ùå Slow (sequential processing)\n",
    "\n",
    "**Rule-based extraction:**\n",
    "- ‚úÖ Fast and cheap\n",
    "- ‚úÖ Deterministic\n",
    "- ‚ùå Limited to simple patterns\n",
    "- ‚ùå Requires domain knowledge\n",
    "\n",
    "**Best Practice**: Use rule-based for simple metadata (dates, counts), LLM for complex metadata (summaries, topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Node Relationships\n",
    "\n",
    "### Understanding Node Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node Relationships:\n",
      "\n",
      "Node 0:\n",
      "  ID: 27bf30bc-9394-47af-9e34-31978560b859\n",
      "  Relationships: [<NodeRelationship.SOURCE: '1'>]\n",
      "  Source Document ID: c8cf197a-472d-4445-9590-c1dd6f6c1eb8\n",
      "\n",
      "Node 1:\n",
      "  ID: e79ff1b8-839f-4d37-9485-25d59d9bbf74\n",
      "  Relationships: [<NodeRelationship.SOURCE: '1'>]\n",
      "  Source Document ID: b94c9437-8352-42c0-9ce0-d5a4da3d43e7\n",
      "\n",
      "Node 2:\n",
      "  ID: 7bb5a46e-710a-4232-aefa-0d3e47705904\n",
      "  Relationships: [<NodeRelationship.SOURCE: '1'>]\n",
      "  Source Document ID: 1d0dfd59-3643-458f-9c23-fe860e5f4e11\n"
     ]
    }
   ],
   "source": [
    "# Inspect node relationships\n",
    "print(\"Node Relationships:\")\n",
    "for i, node in enumerate(sentence_nodes[:3]):\n",
    "    print(f\"\\nNode {i}:\")\n",
    "    print(f\"  ID: {node.node_id}\")\n",
    "    print(f\"  Relationships: {list(node.relationships.keys())}\")\n",
    "    \n",
    "    # Check for source document\n",
    "    if NodeRelationship.SOURCE in node.relationships:\n",
    "        source_info = node.relationships[NodeRelationship.SOURCE]\n",
    "        print(f\"  Source Document ID: {source_info.node_id}\")\n",
    "    \n",
    "    # Check for previous/next nodes\n",
    "    if NodeRelationship.PREVIOUS in node.relationships:\n",
    "        print(f\"  Has PREVIOUS node\")\n",
    "    if NodeRelationship.NEXT in node.relationships:\n",
    "        print(f\"  Has NEXT node\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Custom Node Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created hierarchical relationship:\n",
      "  Summary Node ID: 9f2873a2-4a62-4dc3-ab42-a1a026243e67\n",
      "  Child nodes: 3\n"
     ]
    }
   ],
   "source": [
    "# Create custom parent-child relationships\n",
    "# Example: Create a summary node that links to detail nodes\n",
    "\n",
    "summary_node = TextNode(\n",
    "    text=\"Summary: Research papers on transformers, BERT, and RAG\",\n",
    "    metadata={\"type\": \"summary\", \"level\": \"0\"},\n",
    ")\n",
    "\n",
    "# Link detail nodes as children\n",
    "for node in sentence_nodes[:3]:\n",
    "    node.relationships[NodeRelationship.PARENT] = RelatedNodeInfo(\n",
    "        node_id=summary_node.node_id,\n",
    "    )\n",
    "    node.metadata[\"level\"] = \"1\"\n",
    "\n",
    "print(\"Created hierarchical relationship:\")\n",
    "print(f\"  Summary Node ID: {summary_node.node_id}\")\n",
    "print(f\"  Child nodes: {len([n for n in sentence_nodes[:3] if NodeRelationship.PARENT in n.relationships])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Ingestion Pipeline\n",
    "\n",
    "### Creating a Complete Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ingestion pipeline...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84bf77ffbbe84ee4be13abffc9f70c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaff4d94a1b44b1f9abf2d6475c21fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Pipeline complete!\n",
      "  Processed 3 documents\n",
      "  Generated 3 nodes\n",
      "  Nodes have embeddings: True\n"
     ]
    }
   ],
   "source": [
    "# Build ingestion pipeline\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=1024, chunk_overlap=200),\n",
    "        Settings.embed_model,  # Generate embeddings\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Running ingestion pipeline...\")\n",
    "nodes = pipeline.run(documents=sample_papers, show_progress=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Pipeline complete!\")\n",
    "print(f\"  Processed {len(sample_papers)} documents\")\n",
    "print(f\"  Generated {len(nodes)} nodes\")\n",
    "print(f\"  Nodes have embeddings: {nodes[0].embedding is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Benefits\n",
    "\n",
    "**IngestionPipeline** provides:\n",
    "- ‚úÖ **Caching**: Avoid re-processing unchanged documents\n",
    "- ‚úÖ **Batch processing**: Efficient for large document sets\n",
    "- ‚úÖ **Composable**: Chain multiple transformations\n",
    "- ‚úÖ **Async support**: Parallel processing\n",
    "- ‚úÖ **Error handling**: Graceful failures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Building an Index with Optimized Chunks\n",
    "\n",
    "### Using Our Processed Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Index created from processed nodes\n",
      "  Total nodes indexed: 3\n"
     ]
    }
   ],
   "source": [
    "# Create index from our processed nodes\n",
    "index = VectorStoreIndex(nodes=nodes)\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=3,\n",
    "    response_mode=\"compact\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Index created from processed nodes\")\n",
    "print(f\"  Total nodes indexed: {len(nodes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying with Rich Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the Transformer architecture?\n",
      "\n",
      "Response:\n",
      "The Transformer architecture is a novel network design that relies entirely on attention mechanisms, eliminating the need for recurrence and convolutions. It is specifically developed for sequence transduction tasks and computes representations of input and output through self-attention, distinguishing itself from traditional models that utilize recurrent neural networks or convolutional layers.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Retrieved Sources:\n",
      "\n",
      "Source 1:\n",
      "  Score: 0.4582\n",
      "  Title: Attention Is All You Need\n",
      "  Year: 2017\n",
      "  Category: transformers\n",
      "  Text preview: Title: Attention Is All You Need\n",
      "        Authors: Vaswani et al.\n",
      "        Year: 2017\n",
      "\n",
      "        Abstract: The dominant sequence transduction models are b...\n",
      "\n",
      "Source 2:\n",
      "  Score: 0.2470\n",
      "  Title: BERT\n",
      "  Year: 2019\n",
      "  Category: language_models\n",
      "  Text preview: Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
      "        Authors: Devlin et al.\n",
      "        Year: 2019\n",
      "\n",
      "        Abs...\n",
      "\n",
      "Source 3:\n",
      "  Score: 0.1484\n",
      "  Title: RAG\n",
      "  Year: 2020\n",
      "  Category: rag\n",
      "  Text preview: Title: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
      "        Authors: Lewis et al.\n",
      "        Year: 2020\n",
      "\n",
      "        Abstract: Large pre-...\n"
     ]
    }
   ],
   "source": [
    "# Query about transformers\n",
    "query = \"What is the Transformer architecture?\"\n",
    "response = query_engine.query(query)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Response:\")\n",
    "print(response)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Examine retrieved sources\n",
    "print(\"\\nRetrieved Sources:\")\n",
    "for i, source_node in enumerate(response.source_nodes, 1):\n",
    "    print(f\"\\nSource {i}:\")\n",
    "    print(f\"  Score: {source_node.score:.4f}\")\n",
    "    print(f\"  Title: {source_node.metadata.get('title', 'N/A')}\")\n",
    "    print(f\"  Year: {source_node.metadata.get('year', 'N/A')}\")\n",
    "    print(f\"  Category: {source_node.metadata.get('category', 'N/A')}\")\n",
    "    print(f\"  Text preview: {source_node.text[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Explain retrieval-augmented generation\n",
      "\n",
      "Response:\n",
      "Retrieval-augmented generation (RAG) is a model that integrates both parametric and non-parametric memory to enhance the performance of natural language processing tasks. It combines the capabilities of large pre-trained language models, which store knowledge in their parameters, with a dense vector index of external information sources, such as Wikipedia. This approach allows the model to access up-to-date information and improves its ability to interpret and manipulate knowledge effectively. By leveraging both types of memory, RAG models aim to provide more accurate and contextually relevant responses in knowledge-intensive applications.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Top Source:\n",
      "  Title: RAG\n",
      "  Authors: Lewis et al.\n",
      "  Citations: 3500\n"
     ]
    }
   ],
   "source": [
    "# Query about RAG\n",
    "query2 = \"Explain retrieval-augmented generation\"\n",
    "response2 = query_engine.query(query2)\n",
    "\n",
    "print(f\"Query: {query2}\\n\")\n",
    "print(\"Response:\")\n",
    "print(response2)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(\"\\nTop Source:\")\n",
    "top_source = response2.source_nodes[0]\n",
    "print(f\"  Title: {top_source.metadata.get('title')}\")\n",
    "print(f\"  Authors: {top_source.metadata.get('authors')}\")\n",
    "print(f\"  Citations: {top_source.metadata.get('citations')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Chunking Best Practices\n",
    "\n",
    "### Experiment: Impact of Chunk Size on Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk Size Impact on Retrieval:\n",
      " Chunk Size  Num Nodes Top Score  Response Len\n",
      "        256          3    0.4572           566\n",
      "        512          3    0.4572           736\n",
      "       1024          3    0.4571           660\n",
      "       2048          3    0.4572           713\n"
     ]
    }
   ],
   "source": [
    "# Test different chunk sizes\n",
    "chunk_sizes = [256, 512, 1024, 2048]\n",
    "test_query = \"What are the benefits of attention mechanisms?\"\n",
    "\n",
    "results = []\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "    # Create splitter\n",
    "    splitter = SentenceSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size * 0.2)  # 20% overlap\n",
    "    )\n",
    "    \n",
    "    # Process and index\n",
    "    temp_nodes = splitter.get_nodes_from_documents(sample_papers)\n",
    "    temp_index = VectorStoreIndex.from_documents(\n",
    "        sample_papers,\n",
    "        transformations=[splitter],\n",
    "        show_progress=False\n",
    "    )\n",
    "    \n",
    "    # Query\n",
    "    temp_engine = temp_index.as_query_engine(similarity_top_k=2)\n",
    "    temp_response = temp_engine.query(test_query)\n",
    "    \n",
    "    results.append({\n",
    "        \"Chunk Size\": chunk_size,\n",
    "        \"Num Nodes\": len(temp_nodes),\n",
    "        \"Top Score\": f\"{temp_response.source_nodes[0].score:.4f}\",\n",
    "        \"Response Len\": len(str(temp_response)),\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"\\nChunk Size Impact on Retrieval:\")\n",
    "print(df_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Key Takeaways: Chunking Best Practices\n",
    "\n",
    "1. **Start with 512-1024 tokens** for most applications\n",
    "2. **Use 10-20% overlap** to preserve context across boundaries\n",
    "3. **Choose SentenceSplitter** for general use (respects boundaries)\n",
    "4. **Use SemanticSplitter** for complex documents (worth the cost)\n",
    "5. **Add rich metadata** for better filtering and attribution\n",
    "6. **Test chunk sizes** on your specific data and queries\n",
    "7. **Monitor costs**: Smaller chunks = more nodes = more embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Summary: What You Learned\n",
    "\n",
    "### ‚úÖ Completed Learning Objectives\n",
    "\n",
    "1. **Document Loading**: Used SimpleDirectoryReader and created custom documents\n",
    "2. **Chunking Strategies**: Implemented sentence, token, and semantic chunking\n",
    "3. **Metadata Management**: Added document and node-level metadata, used LLM extraction\n",
    "4. **Node Relationships**: Created parent-child and sequential relationships\n",
    "5. **Optimization**: Built ingestion pipeline, compared chunk sizes\n",
    "6. **Best Practices**: Learned trade-offs and guidelines for production\n",
    "\n",
    "### Key Concepts Mastered\n",
    "\n",
    "- **SentenceSplitter**: Respects sentence boundaries, good default\n",
    "- **TokenTextSplitter**: Precise token control\n",
    "- **SemanticSplitterNodeParser**: Meaning-based chunking (slower, higher quality)\n",
    "- **IngestionPipeline**: Composable, cacheable document processing\n",
    "- **Metadata enrichment**: Rule-based and LLM-based extraction\n",
    "- **Node relationships**: SOURCE, PREVIOUS, NEXT, PARENT, CHILD\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In **Notebook 3: Indexing & Simple Queries**, you'll learn:\n",
    "- Integrating external vector stores (Qdrant, Chroma)\n",
    "- Comparing embedding models (OpenAI vs HuggingFace)\n",
    "- Index persistence and loading\n",
    "- Advanced query engine configuration\n",
    "- Response synthesis strategies\n",
    "- VectorIndexAutoRetriever for smart filtering\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Practice Exercises\n",
    "\n",
    "1. **Load Real PDFs**: Place PDFs in `data/research_papers/` and load with SimpleDirectoryReader\n",
    "2. **Compare Chunking**: Create indexes with different chunk sizes (256, 512, 1024). Which works best for your data?\n",
    "3. **Metadata Extraction**: Add custom metadata extractors for your domain (e.g., extract dates, authors, topics)\n",
    "4. **Semantic Chunking**: Apply SemanticSplitterNodeParser to a long document. How does it split compared to SentenceSplitter?\n",
    "5. **Pipeline**: Build a custom ingestion pipeline with multiple transformations\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- **Node Parsers**: https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/\n",
    "- **Metadata Extractors**: https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/\n",
    "- **Ingestion Pipeline**: https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
